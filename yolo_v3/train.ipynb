{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from ipynb.fs.full.model import YOLOv3\n",
    "from ipynb.fs.full.utils import (\n",
    "    mean_average_precision,\n",
    "    cells_to_bboxes,\n",
    "    get_evaluation_bboxes,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    check_class_accuracy,\n",
    "    get_loaders,\n",
    "    plot_couple_examples\n",
    ")\n",
    "from ipynb.fs.full.loss import YoloLoss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код из config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "DATASET = 'PASCAL_VOC'\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# seed_everything()  # If you want deterministic behavior\n",
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 416\n",
    "NUM_CLASSES = 20\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_EPOCHS = 100\n",
    "CONF_THRESHOLD = 0.05\n",
    "MAP_IOU_THRESH = 0.5\n",
    "NMS_IOU_THRESH = 0.45\n",
    "S = [IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8]\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = True\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_FILE = \"checkpoint.pth.tar\"\n",
    "IMG_DIR = DATASET + \"/images/\"\n",
    "LABEL_DIR = DATASET + \"/labels/\"\n",
    "\n",
    "ANCHORS = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "]  # Note these have been rescaled to be between [0, 1]\n",
    "\n",
    "\n",
    "scale = 1.1\n",
    "train_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=int(IMAGE_SIZE * scale),\n",
    "            min_width=int(IMAGE_SIZE * scale),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "        ),\n",
    "        A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),\n",
    "        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ShiftScaleRotate(\n",
    "                    rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT\n",
    "                ),\n",
    "                A.Affine(shear=15, p=0.5, mode=\"constant\"),\n",
    "            ],\n",
    "            p=1.0,\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Blur(p=0.1),\n",
    "        A.CLAHE(p=0.1),\n",
    "        A.Posterize(p=0.1),\n",
    "        A.ToGray(p=0.1),\n",
    "        A.ChannelShuffle(p=0.05),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
    ")\n",
    "test_transforms = A.Compose(\n",
    "    [\n",
    "        A.LongestMaxSize(max_size=IMAGE_SIZE),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
    ")\n",
    "\n",
    "PASCAL_CLASSES = [\n",
    "    \"aeroplane\",\n",
    "    \"bicycle\",\n",
    "    \"bird\",\n",
    "    \"boat\",\n",
    "    \"bottle\",\n",
    "    \"bus\",\n",
    "    \"car\",\n",
    "    \"cat\",\n",
    "    \"chair\",\n",
    "    \"cow\",\n",
    "    \"diningtable\",\n",
    "    \"dog\",\n",
    "    \"horse\",\n",
    "    \"motorbike\",\n",
    "    \"person\",\n",
    "    \"pottedplant\",\n",
    "    \"sheep\",\n",
    "    \"sofa\",\n",
    "    \"train\",\n",
    "    \"tvmonitor\"\n",
    "]\n",
    "\n",
    "COCO_LABELS = ['person',\n",
    " 'bicycle',\n",
    " 'car',\n",
    " 'motorcycle',\n",
    " 'airplane',\n",
    " 'bus',\n",
    " 'train',\n",
    " 'truck',\n",
    " 'boat',\n",
    " 'traffic light',\n",
    " 'fire hydrant',\n",
    " 'stop sign',\n",
    " 'parking meter',\n",
    " 'bench',\n",
    " 'bird',\n",
    " 'cat',\n",
    " 'dog',\n",
    " 'horse',\n",
    " 'sheep',\n",
    " 'cow',\n",
    " 'elephant',\n",
    " 'bear',\n",
    " 'zebra',\n",
    " 'giraffe',\n",
    " 'backpack',\n",
    " 'umbrella',\n",
    " 'handbag',\n",
    " 'tie',\n",
    " 'suitcase',\n",
    " 'frisbee',\n",
    " 'skis',\n",
    " 'snowboard',\n",
    " 'sports ball',\n",
    " 'kite',\n",
    " 'baseball bat',\n",
    " 'baseball glove',\n",
    " 'skateboard',\n",
    " 'surfboard',\n",
    " 'tennis racket',\n",
    " 'bottle',\n",
    " 'wine glass',\n",
    " 'cup',\n",
    " 'fork',\n",
    " 'knife',\n",
    " 'spoon',\n",
    " 'bowl',\n",
    " 'banana',\n",
    " 'apple',\n",
    " 'sandwich',\n",
    " 'orange',\n",
    " 'broccoli',\n",
    " 'carrot',\n",
    " 'hot dog',\n",
    " 'pizza',\n",
    " 'donut',\n",
    " 'cake',\n",
    " 'chair',\n",
    " 'couch',\n",
    " 'potted plant',\n",
    " 'bed',\n",
    " 'dining table',\n",
    " 'toilet',\n",
    " 'tv',\n",
    " 'laptop',\n",
    " 'mouse',\n",
    " 'remote',\n",
    " 'keyboard',\n",
    " 'cell phone',\n",
    " 'microwave',\n",
    " 'oven',\n",
    " 'toaster',\n",
    " 'sink',\n",
    " 'refrigerator',\n",
    " 'book',\n",
    " 'clock',\n",
    " 'vase',\n",
    " 'scissors',\n",
    " 'teddy bear',\n",
    " 'hair drier',\n",
    " 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ipynb.fs.full.utils import (\n",
    "    cells_to_bboxes,\n",
    "    iou_width_height as iou,\n",
    "    non_max_suppression as nms,\n",
    "    plot_image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_file,\n",
    "        img_dir,\n",
    "        label_dir,\n",
    "        anchors,\n",
    "        image_size=416,\n",
    "        S=[13, 26, 52],\n",
    "        C=20,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        self.C = C\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image=image, bboxes=bboxes)\n",
    "            image = augmentations[\"image\"]\n",
    "            bboxes = augmentations[\"bboxes\"]\n",
    "\n",
    "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
    "        targets = [torch.zeros((self.num_anchors // 3, S, S, 6)) for S in self.S]\n",
    "        for box in bboxes:\n",
    "            iou_anchors = iou(torch.tensor(box[2:4]), self.anchors)\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "            x, y, width, height, class_label = box\n",
    "            has_anchor = [False] * 3  # each scale should have one anchor\n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "                S = self.S[scale_idx]\n",
    "                i, j = int(S * y), int(S * x)  # which cell\n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "                    x_cell, y_cell = S * x - j, S * y - i  # both between [0,1]\n",
    "                    width_cell, height_cell = (\n",
    "                        width * S,\n",
    "                        height * S,\n",
    "                    )  # can be greater than 1 since it's relative to cell\n",
    "                    box_coordinates = torch.tensor(\n",
    "                        [x_cell, y_cell, width_cell, height_cell]\n",
    "                    )\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
    "                    has_anchor[scale_idx] = True\n",
    "\n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
    "\n",
    "        return image, tuple(targets)\n",
    "    \n",
    "def test():\n",
    "    anchors = ANCHORS\n",
    "\n",
    "    transform = test_transforms\n",
    "\n",
    "    dataset = YOLODataset(\n",
    "        \"COCO/train.csv\",\n",
    "        \"COCO/images/images/\",\n",
    "        \"COCO/labels/labels_new/\",\n",
    "        S=[13, 26, 52],\n",
    "        anchors=anchors,\n",
    "        transform=transform,\n",
    "    )\n",
    "    S = [13, 26, 52]\n",
    "    scaled_anchors = torch.tensor(anchors) / (\n",
    "        1 / torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    )\n",
    "    loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)\n",
    "    for x, y in loader:\n",
    "        boxes = []\n",
    "\n",
    "        for i in range(y[0].shape[1]):\n",
    "            anchor = scaled_anchors[i]\n",
    "            print(anchor.shape)\n",
    "            print(y[i].shape)\n",
    "            boxes += cells_to_bboxes(\n",
    "                y[i], is_preds=False, S=y[i].shape[2], anchors=anchor\n",
    "            )[0]\n",
    "        boxes = nms(boxes, iou_threshold=1, threshold=0.7, box_format=\"midpoint\")\n",
    "        print(boxes)\n",
    "        plot_image(x[0].permute(1, 2, 0).to(\"cpu\"), boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код для тренировки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    losses = []\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x = x.to(DEVICE)\n",
    "        y0, y1, y2 = (\n",
    "            y[0].to(DEVICE),\n",
    "            y[1].to(DEVICE),\n",
    "            y[2].to(DEVICE),\n",
    "        )\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(x)\n",
    "            loss = (\n",
    "                loss_fn(out[0], y0, scaled_anchors[0])\n",
    "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
    "                + loss_fn(out[2], y2, scaled_anchors[2])\n",
    "            )\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # update progress bar\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        loop.set_postfix(loss=mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = YOLOv3(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = YoloLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    train_loader, test_loader, train_eval_loader = get_loaders(\n",
    "        train_csv_path=DATASET + \"/train.csv\", test_csv_path=DATASET + \"/test.csv\"\n",
    "    )\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            CHECKPOINT_FILE, model, optimizer, LEARNING_RATE\n",
    "        )\n",
    "\n",
    "    scaled_anchors = (\n",
    "        torch.tensor(ANCHORS)\n",
    "        * torch.tensor(S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        #plot_couple_examples(model, test_loader, 0.6, 0.5, scaled_anchors)\n",
    "        train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
    "\n",
    "        #if config.SAVE_MODEL:\n",
    "        #    save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
    "\n",
    "        #print(f\"Currently epoch {epoch}\")\n",
    "        #print(\"On Train Eval loader:\")\n",
    "        #print(\"On Train loader:\")\n",
    "        #check_class_accuracy(model, train_loader, threshold=config.CONF_THRESHOLD)\n",
    "\n",
    "        if epoch > 0 and epoch % 3 == 0:\n",
    "            check_class_accuracy(model, test_loader, threshold=CONF_THRESHOLD)\n",
    "            pred_boxes, true_boxes = get_evaluation_bboxes(\n",
    "                test_loader,\n",
    "                model,\n",
    "                iou_threshold=NMS_IOU_THRESH,\n",
    "                anchors=ANCHORS,\n",
    "                threshold=CONF_THRESHOLD,\n",
    "            )\n",
    "            mapval = mean_average_precision(\n",
    "                pred_boxes,\n",
    "                true_boxes,\n",
    "                iou_threshold=MAP_IOU_THRESH,\n",
    "                box_format=\"midpoint\",\n",
    "                num_classes=NUM_CLASSES,\n",
    "            )\n",
    "            print(f\"MAP: {mapval.item()}\")\n",
    "            model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
